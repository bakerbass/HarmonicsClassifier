{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15db919e",
   "metadata": {},
   "source": [
    "# Guitar Harmonics CNN Classifier Training\n",
    "\n",
    "This notebook trains a CNN model for classifying guitar harmonics from audio clips using mel spectrograms.\n",
    "\n",
    "**Classes:**\n",
    "- `harmonic`: Natural or artificial harmonics\n",
    "- `dead_note`: Muted or deadened notes\n",
    "- `general_note`: Regular notes\n",
    "\n",
    "**Model Architecture:**\n",
    "- 4 convolutional blocks with BatchNorm, ReLU, MaxPool\n",
    "- Global average pooling\n",
    "- Fully connected classification head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980c6791",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Set Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be0b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02448bf",
   "metadata": {},
   "source": [
    "## 2. Define HarmonicsDataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10398a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HarmonicsDataset(Dataset):\n",
    "    \"\"\"Dataset for guitar harmonics audio clips.\"\"\"\n",
    "    \n",
    "    def __init__(self, metadata_df, sr=22050, duration=3.0, n_mels=128, n_fft=2048, hop_length=512):\n",
    "        self.metadata = metadata_df.reset_index(drop=True)\n",
    "        self.sr = sr\n",
    "        self.duration = duration\n",
    "        self.n_mels = n_mels\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        \n",
    "        # Label mapping\n",
    "        self.label_map = {'harmonic': 0, 'dead_note': 1, 'general_note': 2}\n",
    "        self.labels = [self.label_map[label] for label in self.metadata['label_category']]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.metadata.iloc[idx]\n",
    "        \n",
    "        try:\n",
    "            # Load audio\n",
    "            audio, _ = librosa.load(\n",
    "                row['source_audio'],\n",
    "                sr=self.sr,\n",
    "                offset=row['onset_sec'],\n",
    "                duration=min(row['duration_sec'], self.duration)\n",
    "            )\n",
    "            \n",
    "            # Pad or trim to fixed length\n",
    "            target_length = int(self.sr * self.duration)\n",
    "            if len(audio) < target_length:\n",
    "                audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "            else:\n",
    "                audio = audio[:target_length]\n",
    "            \n",
    "            # Compute mel spectrogram\n",
    "            mel_spec = librosa.feature.melspectrogram(\n",
    "                y=audio,\n",
    "                sr=self.sr,\n",
    "                n_fft=self.n_fft,\n",
    "                hop_length=self.hop_length,\n",
    "                n_mels=self.n_mels,\n",
    "                fmin=80,\n",
    "                fmax=8000\n",
    "            )\n",
    "            \n",
    "            # Convert to log scale (dB)\n",
    "            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            \n",
    "            # Normalize to [0, 1]\n",
    "            mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n",
    "            \n",
    "            # Convert to tensor (add channel dimension)\n",
    "            mel_tensor = torch.FloatTensor(mel_spec_norm).unsqueeze(0)\n",
    "            label = torch.LongTensor([self.labels[idx]])[0]\n",
    "            \n",
    "            return mel_tensor, label\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample {idx}: {e}\")\n",
    "            # Return zeros on error\n",
    "            mel_tensor = torch.zeros((1, self.n_mels, 130))  # Approximate time frames\n",
    "            label = torch.LongTensor([self.labels[idx]])[0]\n",
    "            return mel_tensor, label\n",
    "\n",
    "print(\"HarmonicsDataset class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b5dd73",
   "metadata": {},
   "source": [
    "## 3. Define HarmonicsCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951827e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HarmonicsCNN(nn.Module):\n",
    "    \"\"\"CNN for guitar harmonics classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=3, dropout=0.5):\n",
    "        super(HarmonicsCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.25)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.25)\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.25)\n",
    "        )\n",
    "        \n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))  # Global average pooling\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "print(\"HarmonicsCNN model defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716a2353",
   "metadata": {},
   "source": [
    "## 4. Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0368495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(labels):\n",
    "    \"\"\"Compute class weights for imbalanced dataset.\"\"\"\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    total = len(labels)\n",
    "    weights = total / (len(unique) * counts)\n",
    "    return torch.FloatTensor(weights)\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for inputs, labels in pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': running_loss / (pbar.n + 1), 'acc': 100. * correct / total})\n",
    "    \n",
    "    return running_loss / len(loader), 100. * correct / total\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(loader, desc='Validation'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return running_loss / len(loader), 100. * correct / total, all_preds, all_labels\n",
    "\n",
    "\n",
    "def plot_training_history(history, output_dir):\n",
    "    \"\"\"Plot training and validation metrics.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "    axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1].plot(history['train_acc'], label='Train Acc', linewidth=2)\n",
    "    axes[1].plot(history['val_acc'], label='Val Acc', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title('Training and Validation Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'training_history.png', dpi=150, bbox_inches='tight')\n",
    "    print(f\"Saved: {output_dir / 'training_history.png'}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, output_dir):\n",
    "    \"\"\"Plot confusion matrix.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['harmonic', 'dead_note', 'general_note'],\n",
    "                yticklabels=['harmonic', 'dead_note', 'general_note'])\n",
    "    plt.title('Confusion Matrix - Test Set')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "    print(f\"Saved: {output_dir / 'confusion_matrix.png'}\")\n",
    "    plt.close()\n",
    "\n",
    "print(\"Helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa3410f",
   "metadata": {},
   "source": [
    "## 5. Configure Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d98a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "METADATA_PATH = 'processed_dataset/metadata.csv'\n",
    "OUTPUT_DIR = Path('models/')\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "DROPOUT = 0.5\n",
    "N_SAMPLES_PER_CLASS = None  # Set to an integer to limit samples per class, or None for all\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Metadata: {METADATA_PATH}\")\n",
    "print(f\"  Output: {OUTPUT_DIR}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Dropout: {DROPOUT}\")\n",
    "print(f\"  Samples per class: {N_SAMPLES_PER_CLASS if N_SAMPLES_PER_CLASS else 'All'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa08fa0b",
   "metadata": {},
   "source": [
    "## 6. Load and Prepare Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9cd932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "print(\"Loading metadata...\")\n",
    "df = pd.read_csv(METADATA_PATH)\n",
    "print(f\"Loaded {len(df)} samples\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nDataset overview:\")\n",
    "print(df.head())\n",
    "print(\"\\nColumn names:\", df.columns.tolist())\n",
    "print(\"\\nClass distribution (before limiting):\")\n",
    "print(df['label_category'].value_counts())\n",
    "\n",
    "# Optionally limit samples per class\n",
    "if N_SAMPLES_PER_CLASS:\n",
    "    print(f\"\\nLimiting to {N_SAMPLES_PER_CLASS} samples per class...\")\n",
    "    df_balanced = []\n",
    "    for label in ['harmonic', 'dead_note', 'general_note']:\n",
    "        subset = df[df['label_category'] == label]\n",
    "        sampled = subset.sample(min(N_SAMPLES_PER_CLASS, len(subset)), random_state=42)\n",
    "        df_balanced.append(sampled)\n",
    "    df = pd.concat(df_balanced, ignore_index=True)\n",
    "    print(f\"Dataset size after limiting: {len(df)} samples\")\n",
    "    print(\"\\nClass distribution (after limiting):\")\n",
    "    print(df['label_category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616e8843",
   "metadata": {},
   "source": [
    "## 7. Split Dataset by Audio Files\n",
    "\n",
    "Split by audio files (not individual samples) to prevent data leakage between train/val/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792c428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by audio files to prevent data leakage\n",
    "print(\"Splitting dataset by audio files...\")\n",
    "audio_files = df['source_audio'].unique()\n",
    "print(f\"Total unique audio files: {len(audio_files)}\")\n",
    "\n",
    "# Create file-to-dominant-class mapping for stratification\n",
    "file_labels = {}\n",
    "for audio_file in audio_files:\n",
    "    subset = df[df['source_audio'] == audio_file]\n",
    "    dominant_class = subset['label_category'].mode()[0] if len(subset) > 0 else 'unknown'\n",
    "    file_labels[audio_file] = dominant_class\n",
    "\n",
    "# Split files (70% train, 15% val, 15% test)\n",
    "try:\n",
    "    train_files, test_files = train_test_split(\n",
    "        audio_files, test_size=0.15, stratify=[file_labels[f] for f in audio_files], random_state=42\n",
    "    )\n",
    "    train_files, val_files = train_test_split(\n",
    "        train_files, test_size=0.15/0.85, stratify=[file_labels[f] for f in train_files], random_state=42\n",
    "    )\n",
    "except ValueError:\n",
    "    print(\"Warning: Stratified split failed, using random split\")\n",
    "    train_files, test_files = train_test_split(audio_files, test_size=0.15, random_state=42)\n",
    "    train_files, val_files = train_test_split(train_files, test_size=0.15/0.85, random_state=42)\n",
    "\n",
    "# Create dataset splits\n",
    "train_df = df[df['source_audio'].isin(train_files)]\n",
    "val_df = df[df['source_audio'].isin(val_files)]\n",
    "test_df = df[df['source_audio'].isin(test_files)]\n",
    "\n",
    "print(f\"\\nSplit statistics:\")\n",
    "print(f\"  Train: {len(train_df)} samples from {len(train_files)} files\")\n",
    "print(f\"  Val: {len(val_df)} samples from {len(val_files)} files\")\n",
    "print(f\"  Test: {len(test_df)} samples from {len(test_files)} files\")\n",
    "\n",
    "# Print class distribution\n",
    "print(\"\\nClass distribution per split:\")\n",
    "for split_name, split_df in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:\n",
    "    print(f\"\\n  {split_name}:\")\n",
    "    for label in ['harmonic', 'dead_note', 'general_note']:\n",
    "        count = (split_df['label_category'] == label).sum()\n",
    "        pct = 100 * count / len(split_df)\n",
    "        print(f\"    {label}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6fe5c5",
   "metadata": {},
   "source": [
    "## 8. Create PyTorch Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2c0281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "print(\"Creating PyTorch datasets...\")\n",
    "train_dataset = HarmonicsDataset(train_df)\n",
    "val_dataset = HarmonicsDataset(val_df)\n",
    "test_dataset = HarmonicsDataset(test_df)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Val dataset: {len(val_dataset)} samples\")\n",
    "print(f\"Test dataset: {len(test_dataset)} samples\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"\\nDataLoaders created:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d906fd0",
   "metadata": {},
   "source": [
    "## 9. Initialize Model, Loss, and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1027f5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights for imbalanced dataset\n",
    "class_weights = compute_class_weights(train_dataset.labels).to(device)\n",
    "print(f\"Class weights: {class_weights.cpu().numpy()}\")\n",
    "\n",
    "# Create model\n",
    "print(\"\\nCreating model...\")\n",
    "model = HarmonicsCNN(num_classes=3, dropout=DROPOUT).to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Print model architecture\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Loss function with class weights\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nModel, loss, and optimizer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f8858",
   "metadata": {},
   "source": [
    "## 10. Training Loop\n",
    "\n",
    "Train the model for the specified number of epochs, tracking metrics and saving the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0113d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, val_preds, val_labels = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Compute F1 score\n",
    "    val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, Val F1: {val_f1:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch + 1\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'val_f1': val_f1,\n",
    "        }, OUTPUT_DIR / 'best_model.pt')\n",
    "        print(f\"✓ Saved best model (val_acc: {val_acc:.2f}%)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training Complete!\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.2f}% at epoch {best_epoch}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3f62d0",
   "metadata": {},
   "source": [
    "## 11. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a67d155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "print(\"Plotting training history...\")\n",
    "plot_training_history(history, OUTPUT_DIR)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baf088b",
   "metadata": {},
   "source": [
    "## 12. Evaluate on Test Set\n",
    "\n",
    "Load the best model and evaluate on the held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de71f380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on best model\n",
    "print(\"=\"*60)\n",
    "print(\"TESTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load(OUTPUT_DIR / 'best_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch'] + 1}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc, test_preds, test_labels = validate(model, test_loader, criterion, device)\n",
    "test_f1 = f1_score(test_labels, test_preds, average='macro')\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Test Results:\")\n",
    "print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "print(f\"  Test Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"  Test F1 Score (macro): {test_f1:.4f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2841cc8",
   "metadata": {},
   "source": [
    "## 13. Generate Classification Report and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032d1ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(test_labels, test_preds, \n",
    "                          target_names=['harmonic', 'dead_note', 'general_note']))\n",
    "\n",
    "# Plot confusion matrix\n",
    "print(\"\\nPlotting confusion matrix...\")\n",
    "plot_confusion_matrix(test_labels, test_preds, OUTPUT_DIR)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7dc446",
   "metadata": {},
   "source": [
    "## 14. Save Results\n",
    "\n",
    "Save all training results and configuration to a JSON file for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cbc858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "results = {\n",
    "    'test_accuracy': float(test_acc),\n",
    "    'test_f1_macro': float(test_f1),\n",
    "    'test_loss': float(test_loss),\n",
    "    'best_val_accuracy': float(best_val_acc),\n",
    "    'best_epoch': int(best_epoch),\n",
    "    'total_epochs': EPOCHS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'dropout': DROPOUT,\n",
    "    'n_samples_per_class': N_SAMPLES_PER_CLASS,\n",
    "}\n",
    "\n",
    "results_path = OUTPUT_DIR / 'results.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"✓ Results saved to {results_path}\")\n",
    "print(\"\\nFinal Results Summary:\")\n",
    "print(json.dumps(results, indent=2))\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
